{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('./data/BTCUSDT_1m.csv')\n",
    "\n",
    "# Display summary statistics\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "df = df.sort_values(by='open_time', ascending=True).reset_index(drop=True)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get memory usage of the dataframe\n",
    "memory_usage = df.memory_usage(deep=True)\n",
    "\n",
    "# Convert to MB for better readability\n",
    "memory_usage_mb = memory_usage.sum() / 1024 / 1024\n",
    "\n",
    "print(\"\\nMemory usage breakdown by column (bytes):\")\n",
    "for column, usage in memory_usage.items():\n",
    "    print(f\"{column}: {usage:,} bytes\")\n",
    "    \n",
    "print(f\"\\nTotal memory usage: {memory_usage_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping old column names to new ones with '1m_' prefix\n",
    "rename_dict = {\n",
    "    'open_time': '1m_open_time',\n",
    "    'close_time': '1m_close_time', \n",
    "    'open_px': '1m_open_px',\n",
    "    'high_px': '1m_high_px',\n",
    "    'low_px': '1m_low_px',\n",
    "    'close_px': '1m_close_px',\n",
    "    'number_of_trades': '1m_number_of_trades',\n",
    "    'base_asset_volume': '1m_base_asset_volume',\n",
    "    'quote_asset_volume': '1m_quote_asset_volume',\n",
    "    'taker_buy_base_asset_volume': '1m_taker_buy_base_asset_volume',\n",
    "    'taker_buy_quote_asset_volume': '1m_taker_buy_quote_asset_volume'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# Display the new column names\n",
    "print(\"New column names:\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['1m_open_time', '1m_close_time', '1m_open_px', '1m_high_px', '1m_low_px', '1m_close_px']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-minute aggregations\n",
    "# First create a group index (0-4) that repeats every 5 rows\n",
    "df['group_idx'] = df.index % 5\n",
    "\n",
    "# Create is_5m_closed column - True for last entry in each group\n",
    "df['is_5m_closed'] = df['group_idx'] == 4\n",
    "\n",
    "# Group by integer division of index to get 5-minute groups\n",
    "groups = df.groupby(df.index // 5)\n",
    "\n",
    "# Create 5m columns - using cummax/cummin within groups for high/low prices\n",
    "df['5m_open_time'] = groups['1m_open_time'].transform('first')\n",
    "df['5m_close_time'] = groups['1m_close_time'].transform('last') \n",
    "df['5m_open_px'] = groups['1m_open_px'].transform('first')\n",
    "df['5m_high_px'] = groups['1m_high_px'].transform('cummax')\n",
    "df['5m_low_px'] = groups['1m_low_px'].transform('cummin')\n",
    "df['5m_close_px'] = df['1m_close_px']\n",
    "\n",
    "# Drop temporary grouping column\n",
    "df = df.drop('group_idx', axis=1)\n",
    "\n",
    "# Display first few rows to verify\n",
    "df[['5m_open_time', '5m_close_time', '5m_open_px', '5m_high_px', '5m_low_px', '5m_close_px', 'is_5m_closed']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 15-minute aggregations from 1m data\n",
    "df['group_idx'] = df.index % 15\n",
    "\n",
    "# Create is_15m_closed column - True for last entry in each group\n",
    "df['is_15m_closed'] = df['group_idx'] == 14\n",
    "\n",
    "# Group by integer division of index to get 5-minute groups\n",
    "groups = df.groupby(df.index // 15)\n",
    "\n",
    "# Create 15m columns - using cummax/cummin within groups for high/low prices\n",
    "df['15m_open_time'] = groups['1m_open_time'].transform('first')\n",
    "df['15m_close_time'] = groups['1m_close_time'].transform('last') \n",
    "df['15m_open_px'] = groups['1m_open_px'].transform('first')\n",
    "df['15m_high_px'] = groups['1m_high_px'].transform('cummax')\n",
    "df['15m_low_px'] = groups['1m_low_px'].transform('cummin')\n",
    "df['15m_close_px'] = df['1m_close_px']\n",
    "\n",
    "# Drop temporary grouping column\n",
    "df = df.drop('group_idx', axis=1)\n",
    "\n",
    "# Display first few rows to verify\n",
    "df[['15m_open_time', '15m_close_time', '15m_open_px', '15m_high_px', '15m_low_px', '15m_close_px', 'is_15m_closed']].head(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1-hour aggregations from 1m data\n",
    "df['group_idx'] = df.index % 60\n",
    "\n",
    "# Create is_5m_closed column - True for last entry in each group\n",
    "df['is_1h_closed'] = df['group_idx'] == 59\n",
    "\n",
    "# Group by integer division of index to get 5-minute groups\n",
    "groups = df.groupby(df.index // 60)\n",
    "\n",
    "# Create 5m columns - using cummax/cummin within groups for high/low prices\n",
    "df['1h_open_time'] = groups['1m_open_time'].transform('first')\n",
    "df['1h_close_time'] = groups['1m_close_time'].transform('last') \n",
    "df['1h_open_px'] = groups['1m_open_px'].transform('first')\n",
    "df['1h_high_px'] = groups['1m_high_px'].transform('cummax')\n",
    "df['1h_low_px'] = groups['1m_low_px'].transform('cummin')\n",
    "df['1h_close_px'] = df['1m_close_px']\n",
    "\n",
    "# Drop temporary grouping column\n",
    "df = df.drop('group_idx', axis=1)\n",
    "\n",
    "# Display first few rows to verify\n",
    "df[['1h_open_time', '1h_close_time', '1h_open_px', '1h_high_px', '1h_low_px', '1h_close_px', 'is_1h_closed']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 4-hour aggregations from 1m data\n",
    "df['group_idx'] = df.index % 240\n",
    "\n",
    "# Create is_4h_closed column - True for last entry in each group\n",
    "df['is_4h_closed'] = df['group_idx'] == 239\n",
    "\n",
    "groups = df.groupby(df.index // 240)\n",
    "\n",
    "df['4h_open_time'] = groups['1m_open_time'].transform('first')\n",
    "df['4h_close_time'] = groups['1m_close_time'].transform('last') \n",
    "df['4h_open_px'] = groups['1m_open_px'].transform('first')\n",
    "df['4h_high_px'] = groups['1m_high_px'].transform('cummax')\n",
    "df['4h_low_px'] = groups['1m_low_px'].transform('cummin')\n",
    "df['4h_close_px'] = df['1m_close_px']\n",
    "\n",
    "# Drop temporary grouping column\n",
    "df = df.drop('group_idx', axis=1)\n",
    "\n",
    "# Display first few rows to verify\n",
    "df[['4h_open_time', '4h_close_time', '4h_open_px', '4h_high_px', '4h_low_px', '4h_close_px', 'is_4h_closed']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1-day aggregations from 1m data\n",
    "df['group_idx'] = df.index % 1440\n",
    "\n",
    "# Create is_1d_closed column - True for last entry in each group\n",
    "df['is_1d_closed'] = df['group_idx'] == 1439\n",
    "\n",
    "groups = df.groupby(df.index // 1440)\n",
    "\n",
    "df['1d_open_time'] = groups['1m_open_time'].transform('first')\n",
    "df['1d_close_time'] = groups['1m_close_time'].transform('last') \n",
    "df['1d_open_px'] = groups['1m_open_px'].transform('first')\n",
    "df['1d_high_px'] = groups['1m_high_px'].transform('cummax')\n",
    "df['1d_low_px'] = groups['1m_low_px'].transform('cummin')\n",
    "df['1d_close_px'] = df['1m_close_px']\n",
    "\n",
    "df = df.drop('group_idx', axis=1)\n",
    "\n",
    "df[['1d_open_time', '1d_close_time', '1d_open_px', '1d_high_px', '1d_low_px', '1d_close_px', 'is_1d_closed']].head(15)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   1712440.00\n",
       "mean          0.00\n",
       "std           0.04\n",
       "min          -1.00\n",
       "25%           0.00\n",
       "50%           0.00\n",
       "75%           0.00\n",
       "max           1.00\n",
       "Name: signal_1, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> e314377 (update notebook)
   "source": [
    "# Strategy 1, Buy on 0:00, Sell on 12:00\n",
    "df[\"signal_1\"] = 0\n",
    "df.loc[df[\"1m_open_time\"] % 86400000 == 0, \"signal_1\"] = 1\n",
    "df.loc[df[\"1m_open_time\"] % 86400000 == 43200000, \"signal_1\"] = -1\n",
    "# Vectorized calculation of asset positions\n",
    "df[\"asset_1_usdt\"] = 1000\n",
    "df[\"asset_1_btc\"] = 0\n",
    "\n",
    "df[\"signal_1\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = len(df)\n",
    "usdt_list = [0] * n\n",
    "btc_list = [0] * n\n",
    "portfolio_list = [0] * n\n",
    "\n",
    "usdt_list[0] = 1000\n",
    "btc_list[0] = 0\n",
    "portfolio_list[0] = 1000\n",
    "\n",
    "\n",
    "n_trade_usdt = 1000\n",
    "\n",
    "for i in range(len(df) - 1):\n",
    "    n_btc = btc_list[i]\n",
    "    n_usdt = usdt_list[i]\n",
    "    price = df[\"1m_close_px\"].iloc[i]\n",
    "    signal = df[\"signal_1\"].iloc[i]\n",
    "\n",
    "    if signal == 1:\n",
    "        n_usdt = n_usdt - n_trade_usdt\n",
    "        n_btc = n_btc + n_trade_usdt / price\n",
    "    elif signal == -1:\n",
    "        n_usdt = n_usdt + n_trade_usdt\n",
    "        n_btc = n_btc - n_trade_usdt / price\n",
    "    elif signal == 0:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Invalid signal\")\n",
    "    \n",
    "    usdt_list[i + 1] = n_usdt\n",
    "    btc_list[i + 1] = n_btc\n",
    "    \n",
    "df[\"btc_1\"] = btc_list\n",
    "df[\"usdt_1\"] = usdt_list\n",
    "df[\"portfolio_1_open_px\"] = df[\"btc_1\"] * df[\"1m_open_px\"] + df[\"usdt_1\"]\n",
    "df[\"portfolio_1_high_px\"] = df[\"btc_1\"] * df[\"1m_high_px\"] + df[\"usdt_1\"]\n",
    "df[\"portfolio_1_low_px\"] = df[\"btc_1\"] * df[\"1m_low_px\"] + df[\"usdt_1\"]\n",
    "df[\"portfolio_1_close_px\"] = df[\"btc_1\"] * df[\"1m_close_px\"] + df[\"usdt_1\"]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   1712440.00\n",
       "mean       1123.62\n",
       "std         367.74\n",
       "min         581.75\n",
       "25%         849.99\n",
       "50%         955.88\n",
       "75%        1383.81\n",
       "max        2067.70\n",
       "Name: portfolio_1, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> e314377 (update notebook)
   "source": [
    "# Create 1-day aggregations from 1m data\n",
    "df['group_idx'] = df.index % 1440\n",
    "\n",
    "groups = df.groupby(df.index // 1440)\n",
    "\n",
    "# Aggregate portfolio values to daily level\n",
    "df['1d_portfolio_1_open'] = groups['portfolio_1_open_px'].transform('first')\n",
    "df['1d_portfolio_1_high'] = groups['portfolio_1_high_px'].transform('max')\n",
    "df['1d_portfolio_1_low'] = groups['portfolio_1_low_px'].transform('min')\n",
    "df['1d_portfolio_1_close'] = groups['portfolio_1_close_px'].transform('last')\n",
    "\n",
    "df = df.drop('group_idx', axis=1)\n",
    "\n",
    "df[['1d_portfolio_1_open', '1d_portfolio_1_high', '1d_portfolio_1_low', '1d_portfolio_1_close']].describe()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['1d_portfolio_1_open', '1d_portfolio_1_high', '1d_portfolio_1_low',\\n       '1d_portfolio_1_close'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m start_time = \u001b[32m1640995200000\u001b[39m  \u001b[38;5;66;03m# Jan 1, 2022\u001b[39;00m\n\u001b[32m      3\u001b[39m end_time = \u001b[32m1643673600000\u001b[39m   \u001b[38;5;66;03m# Jan 31, 2022\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_portfolio = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1d_portfolio_1_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1d_portfolio_1_high\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1d_portfolio_1_low\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1d_portfolio_1_close\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m df_portfolio = df_portfolio[df_portfolio[\u001b[33m'\u001b[39m\u001b[33mis_1d_closed\u001b[39m\u001b[33m'\u001b[39m] == \u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Filter data for January\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/BullionBear/solvexity/venv/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/BullionBear/solvexity/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/BullionBear/solvexity/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['1d_portfolio_1_open', '1d_portfolio_1_high', '1d_portfolio_1_low',\\n       '1d_portfolio_1_close'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> e314377 (update notebook)
   "source": [
    "# Select January 2022 data (first month in the dataset based on context)\n",
    "start_time = 1640995200000  # Jan 1, 2022\n",
    "end_time = 1643673600000   # Jan 31, 2022\n",
    "\n",
    "df_portfolio = df[['1d_portfolio_1_open', '1d_portfolio_1_high', '1d_portfolio_1_low', '1d_portfolio_1_close']]\n",
    "\n",
    "#df_portfolio = df_portfolio[df_portfolio['is_1d_closed'] == True]\n",
    "# Filter data for January\n",
    "# jan_portfolio = df_portfolio[(df_portfolio['1d_open_time'] >= start_time) & (df_portfolio['1d_open_time'] < end_time)]\n",
    "# \n",
    "# \n",
    "# # Plot candlestick chart\n",
    "# import mplfinance as mpf\n",
    "# \n",
    "# # Create a pandas DataFrame with proper datetime index\n",
    "# jan_portfolio_df = jan_portfolio.copy()\n",
    "# jan_portfolio_df.index = pd.date_range(start='2022-01-01', periods=len(jan_portfolio_df), freq='D')\n",
    "# \n",
    "# # Plot using mplfinance\n",
    "# mpf.plot(jan_portfolio_df, \n",
    "#          type='candle',\n",
    "#          title='Portfolio Performance - January 2022',\n",
    "#          ylabel='Portfolio Value (USDT)',\n",
    "#          volume=False,\n",
    "#          style='yahoo')\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
